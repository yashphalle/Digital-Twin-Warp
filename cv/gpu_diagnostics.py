#!/usr/bin/env python3
"""
GPU Diagnostics Script for Warehouse Tracking System
Comprehensive GPU debugging and performance analysis
"""

import torch
import sys
import os
import subprocess
import logging
from datetime import datetime

# Set up logging
logging.basicConfig(level=logging.INFO, format='%(levelname)s: %(message)s')
logger = logging.getLogger(__name__)

def check_nvidia_driver():
    """Check NVIDIA driver installation"""
    logger.info("üîç CHECKING NVIDIA DRIVER...")
    try:
        result = subprocess.run(['nvidia-smi'], capture_output=True, text=True, timeout=10)
        if result.returncode == 0:
            logger.info("‚úÖ NVIDIA Driver installed and working")
            # Extract driver version
            lines = result.stdout.split('\n')
            for line in lines:
                if 'Driver Version:' in line:
                    logger.info(f"üîç {line.strip()}")
            return True
        else:
            logger.error("‚ùå nvidia-smi failed")
            return False
    except FileNotFoundError:
        logger.error("‚ùå nvidia-smi not found - NVIDIA drivers not installed")
        return False
    except subprocess.TimeoutExpired:
        logger.error("‚ùå nvidia-smi timeout")
        return False

def check_cuda_installation():
    """Check CUDA installation"""
    logger.info("\nüîç CHECKING CUDA INSTALLATION...")
    
    # Check PyTorch CUDA
    logger.info(f"üîç PyTorch Version: {torch.__version__}")
    logger.info(f"üîç PyTorch CUDA Available: {torch.cuda.is_available()}")
    logger.info(f"üîç PyTorch CUDA Version: {torch.version.cuda}")
    logger.info(f"üîç PyTorch cuDNN Version: {torch.backends.cudnn.version()}")
    
    if torch.version.cuda is None:
        logger.error("‚ùå CRITICAL: PyTorch compiled WITHOUT CUDA!")
        logger.error("üí° SOLUTION: Reinstall PyTorch with CUDA:")
        logger.error("   pip uninstall torch torchvision torchaudio")
        logger.error("   pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118")
        return False
    
    return torch.cuda.is_available()

def check_gpu_devices():
    """Check available GPU devices"""
    logger.info("\nüîç CHECKING GPU DEVICES...")
    
    if not torch.cuda.is_available():
        logger.error("‚ùå No CUDA devices available")
        return False
    
    device_count = torch.cuda.device_count()
    logger.info(f"üîç CUDA Device Count: {device_count}")
    
    for i in range(device_count):
        props = torch.cuda.get_device_properties(i)
        logger.info(f"üîç Device {i}: {props.name}")
        logger.info(f"   Total Memory: {props.total_memory / 1024**3:.2f} GB")
        logger.info(f"   Compute Capability: {props.major}.{props.minor}")
        logger.info(f"   Multi-processors: {props.multi_processor_count}")
    
    return True

def test_gpu_allocation():
    """Test GPU memory allocation"""
    logger.info("\nüîç TESTING GPU ALLOCATION...")
    
    if not torch.cuda.is_available():
        logger.error("‚ùå CUDA not available for allocation test")
        return False
    
    try:
        # Test small allocation
        logger.info("üîç Testing small allocation (100MB)...")
        test_tensor = torch.randn(100, 1000, 1000).cuda()
        logger.info("‚úÖ Small allocation successful")
        del test_tensor
        torch.cuda.empty_cache()
        
        # Test larger allocation (1GB)
        logger.info("üîç Testing larger allocation (1GB)...")
        test_tensor = torch.randn(1000, 1000, 1000).cuda()
        logger.info("‚úÖ Large allocation successful")
        del test_tensor
        torch.cuda.empty_cache()
        
        return True
        
    except Exception as e:
        logger.error(f"‚ùå GPU allocation failed: {e}")
        return False

def test_grounding_dino_model():
    """Test Grounding DINO model loading"""
    logger.info("\nüîç TESTING GROUNDING DINO MODEL...")
    
    try:
        from transformers import AutoProcessor, AutoModelForZeroShotObjectDetection
        
        model_id = "IDEA-Research/grounding-dino-base"
        logger.info(f"üîç Loading model: {model_id}")
        
        # Load processor
        processor = AutoProcessor.from_pretrained(model_id)
        logger.info("‚úÖ Processor loaded")
        
        # Load model
        model = AutoModelForZeroShotObjectDetection.from_pretrained(model_id)
        logger.info("‚úÖ Model loaded")
        
        if torch.cuda.is_available():
            logger.info("üîç Moving model to GPU...")
            model = model.to('cuda')
            
            # Verify device placement
            model_device = next(model.parameters()).device
            logger.info(f"üîç Model device: {model_device}")
            
            if str(model_device) == 'cuda:0':
                logger.info("‚úÖ Model successfully on GPU")
                
                # Check memory usage
                memory_used = torch.cuda.memory_allocated(0) / 1024**3
                logger.info(f"üîç GPU Memory used by model: {memory_used:.2f}GB")
                
                return True
            else:
                logger.error(f"‚ùå Model not on GPU: {model_device}")
                return False
        else:
            logger.warning("‚ö†Ô∏è CUDA not available, model on CPU")
            return False
            
    except Exception as e:
        logger.error(f"‚ùå Grounding DINO test failed: {e}")
        return False

def analyze_memory_requirements():
    """Analyze memory requirements for multi-camera setup"""
    logger.info("\nüîç ANALYZING MEMORY REQUIREMENTS...")
    
    if not torch.cuda.is_available():
        logger.error("‚ùå CUDA not available for memory analysis")
        return
    
    total_memory = torch.cuda.get_device_properties(0).total_memory / 1024**3
    logger.info(f"üîç Total GPU Memory: {total_memory:.2f}GB")
    
    # Estimate Grounding DINO memory usage
    estimated_model_size = 6.0  # GB (approximate for grounding-dino-base)
    estimated_per_frame = 0.5   # GB per frame processing
    
    logger.info(f"üîç Estimated model size: {estimated_model_size:.1f}GB")
    logger.info(f"üîç Estimated per-frame processing: {estimated_per_frame:.1f}GB")
    
    # Calculate camera capacity
    available_for_frames = total_memory - estimated_model_size - 1.0  # 1GB buffer
    max_cameras = int(available_for_frames / estimated_per_frame)
    
    logger.info(f"üîç Available memory for frames: {available_for_frames:.1f}GB")
    logger.info(f"üîç Estimated max cameras (parallel): {max_cameras}")
    
    if max_cameras < 11:
        logger.warning(f"‚ö†Ô∏è GPU may not handle all 11 cameras simultaneously")
        logger.warning(f"üí° Consider sequential processing or frame skipping")
    else:
        logger.info(f"‚úÖ GPU should handle all 11 cameras")

def main():
    """Run comprehensive GPU diagnostics"""
    logger.info("üöÄ STARTING GPU DIAGNOSTICS FOR WAREHOUSE TRACKING SYSTEM")
    logger.info("=" * 70)
    logger.info(f"üîç Timestamp: {datetime.now()}")
    logger.info(f"üîç Python Version: {sys.version}")
    logger.info(f"üîç Platform: {sys.platform}")
    
    results = {}
    
    # Run all diagnostic tests
    results['nvidia_driver'] = check_nvidia_driver()
    results['cuda_installation'] = check_cuda_installation()
    results['gpu_devices'] = check_gpu_devices()
    results['gpu_allocation'] = test_gpu_allocation()
    results['grounding_dino'] = test_grounding_dino_model()
    
    # Memory analysis
    analyze_memory_requirements()
    
    # Summary
    logger.info("\n" + "=" * 70)
    logger.info("üìä DIAGNOSTIC SUMMARY:")
    
    all_passed = True
    for test, passed in results.items():
        status = "‚úÖ PASS" if passed else "‚ùå FAIL"
        logger.info(f"   {test.replace('_', ' ').title()}: {status}")
        if not passed:
            all_passed = False
    
    if all_passed:
        logger.info("\nüéâ ALL TESTS PASSED - GPU should work correctly!")
    else:
        logger.info("\n‚ö†Ô∏è SOME TESTS FAILED - GPU issues detected")
        logger.info("üí° Check the error messages above for solutions")
    
    logger.info("=" * 70)

if __name__ == "__main__":
    main()
